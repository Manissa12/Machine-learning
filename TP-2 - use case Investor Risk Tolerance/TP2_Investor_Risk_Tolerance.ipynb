{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TP2 - Investor Risk Tolerance - ML Practical"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup & Context\n",
                "\n",
                "**Scenario**\n",
                "\n",
                "We want to automate part of the portfolio management process by predicting a client's true risk tolerance from demographic, financial, and behavioral data (rather than relying solely on self-reported questionnaires). We'll use the Federal Reserve's Survey of Consumer Finances (SCF) panel, which has the same households in 2007 (pre-crisis) and 2009 (post-crisis).\n",
                "\n",
                "**Key idea (target variable):**\n",
                "\n",
                "Compute risk tolerance as the share of risky assets in total financial assets. Because 2009 market levels were different, we normalize 2009 risky assets by the ratio of average S&P 500 levels in 2009 vs 2007. Then we identify \"intelligent\" investors—those whose risk tolerance changed by < 10% between 2007 and 2009—and define TrueRiskTolerance as their average of 2007 and 2009 risk tolerances.\n",
                "\n",
                "**Questions:**\n",
                "- Why might questionnaire-based risk tolerance be unreliable during crises?\n",
                "- What is the business advantage of algorithmically inferring risk tolerance from behavior?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1 — Environment & Data Loading"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.1 Install & Import"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install pandas scikit-learn matplotlib seaborn streamlit --quiet\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import copy\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "sns.set(style=\"whitegrid\")\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "from pathlib import Path\n",
                "\n",
                "# Modeling\n",
                "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "\n",
                "# Regressors\n",
                "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
                "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
                "from sklearn.svm import SVR"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Load the SCF panel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATAFILE = Path(\"SCFP2009panel.xlsx\")\n",
                "assert DATAFILE.exists(), \"Put SCFP2009panel.xlsx in this folder.\"\n",
                "dataset = pd.read_excel(DATAFILE)\n",
                "dataset.shape, type(dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.3 Quick peek"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset.head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Questions:**\n",
                "- How many rows and columns do you see?\n",
                "- What does a single row represent in business terms?\n",
                "- Which columns look like potential \"leaky\" features (e.g., 2009 variables) if we want a 2007-based predictor?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2 — Build the Target: \"TrueRiskTolerance\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Compute risky / risk-free buckets and 2007/2009 risk tolerance & missing values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Average SP500 during 2007 and 2009\n",
                "# used to normalize 2009 risky assets\n",
                "Average_SP500_2007 = 1478\n",
                "Average_SP500_2009 = 948\n",
                "\n",
                "# Risk-free and risky assets (2007)\n",
                "dataset['RiskFree07'] = dataset['LIQ07'] + dataset['CDS07'] + dataset['SAVBND07'] + dataset['CASHLI07']\n",
                "dataset['Risky07'] = dataset['NMMF07'] + dataset['STOCKS07'] + dataset['BOND07']\n",
                "dataset['RT07'] = dataset['Risky07'] / (dataset['Risky07'] + dataset['RiskFree07'])\n",
                "\n",
                "# Risk-free and risky assets (2009)\n",
                "dataset['RiskFree09'] = dataset['LIQ09'] + dataset['CDS09'] + dataset['SAVBND09'] + dataset['CASHLI09']\n",
                "dataset['Risky09'] = dataset['NMMF09'] + dataset['STOCKS09'] + dataset['BOND09']\n",
                "dataset['RT09'] = dataset['Risky09'] / (dataset['Risky09'] + dataset['RiskFree09']) * \\\n",
                "                  (Average_SP500_2009 / Average_SP500_2007)\n",
                "\n",
                "dataset2 = copy.deepcopy(dataset)\n",
                "dataset2['PercentageChange'] = np.abs(dataset2['RT09'] / dataset2['RT07'] - 1)\n",
                "dataset2.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dealing with missing values\n",
                "\n",
                "# Checking for any null values and removing the null values\n",
                "print('Null Values =', dataset2.isnull().values.any())\n",
                "# Drop the rows containing NA\n",
                "dataset2 = dataset2.dropna(axis=0)\n",
                "\n",
                "dataset2 = dataset2[~dataset2.isin([np.nan, np.inf, -np.inf]).any(axis=1)]\n",
                "\n",
                "# Checking for any null values and removing the null values\n",
                "print('Null Values =', dataset2.isnull().values.any())\n",
                "dataset2.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Inspect distributions (visual intuition)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "sns.histplot(dataset2[\"RT07\"].clip(0, 1), bins=30, ax=axes[0])\n",
                "axes[0].set_title(\"Risk tolerance 2007\")\n",
                "sns.histplot(dataset2[\"RT09\"].clip(0, 1), bins=30, ax=axes[1])\n",
                "axes[1].set_title(\"Risk tolerance 2009 (normalized)\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 \"Intelligent\" investors and the final target"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset3 = copy.deepcopy(dataset2)\n",
                "dataset3['TrueRiskTolerance'] = (dataset3['RT07'] + dataset3['RT09']) / 2\n",
                "dataset3.drop(labels=['RT07', 'RT09'], axis=1, inplace=True)\n",
                "dataset3.drop(labels=['PercentageChange'], axis=1, inplace=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Questions:**\n",
                "- What business behavior does \"PercentageChange ≤ 10%\" capture?\n",
                "- Why do we clip risk tolerance to [0,1]?\n",
                "- Looking at the two histograms, what crisis-era behavioral shift do you observe?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3 — Feature Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "keep = [\"AGE07\", \"EDCL07\", \"MARRIED07\", \"KIDS07\", \"OCCAT107\", \"INCOME07\", \"RISK07\", \"NETWORTH07\", \"TrueRiskTolerance\"]\n",
                "# If your file uses alternate names, remap them first using COLS dict.\n",
                "Xy = dataset3[keep].dropna().copy()\n",
                "X = Xy.drop(columns=[\"TrueRiskTolerance\"])\n",
                "y = Xy[\"TrueRiskTolerance\"]\n",
                "X.shape, y.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Quick sanity visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "corr = Xy.corr(numeric_only=True)\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"vlag\")\n",
                "plt.title(\"Correlation matrix (features and target)\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scatterplot Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pandas.plotting import scatter_matrix\n",
                "plt.figure(figsize=(15, 15))\n",
                "scatter_matrix(Xy, figsize=(14, 14))\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Questions:**\n",
                "- Which features correlate positively with TrueRiskTolerance? Which correlate negatively?\n",
                "- Why do we exclude all 2009 variables from features?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 4 — Train/Test Split & Baselines"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Expect dataset3 to already exist from earlier parts, with the 2007 features + TrueRiskTolerance\n",
                "Y = dataset3[\"TrueRiskTolerance\"].astype(float)\n",
                "X = dataset3.drop(columns=[\"TrueRiskTolerance\"]).copy()\n",
                "validation_size = 0.20\n",
                "seed = 3\n",
                "X_train, X_validation, Y_train, Y_validation = train_test_split(\n",
                "    X, Y, test_size=validation_size, random_state=seed\n",
                ")\n",
                "len(X_train), len(X_validation)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Baseline: predict the training mean"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_mean = Y_train.mean()\n",
                "mae_baseline = mean_absolute_error(Y_validation, np.full_like(Y_validation, y_mean))\n",
                "rmse_baseline = mean_squared_error(Y_validation, np.full_like(Y_validation, y_mean))\n",
                "r2_baseline = r2_score(Y_validation, np.full_like(Y_validation, y_mean))\n",
                "mae_baseline, rmse_baseline, r2_baseline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Questions:**\n",
                "- Why compute a naïve baseline?\n",
                "- Which metric (MAE, RMSE, R²) would you prioritize for this business task, and why?\n",
                "- What does R² negation mean?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 CV options & metric & Compare models\n",
                "\n",
                "We'll use 10-fold CV and R² as the metric (consistent with the original case study; RMSE/MAE would also be fine).\n",
                "We'll include linear, distance-based, tree, and ensemble models. For models that are sensitive to feature scaling (Lasso, ElasticNet, KNN, SVR), we'll wrap them in a Pipeline with StandardScaler for a fair comparison."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import KFold\n",
                "num_folds = 10\n",
                "scoring = \"r2\"\n",
                "cv = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
                "\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
                "from sklearn.neighbors import KNeighborsRegressor\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from sklearn.svm import SVR\n",
                "from sklearn.ensemble import (\n",
                "    AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
                ")\n",
                "from sklearn.model_selection import cross_val_score\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "models = [\n",
                "    (\"LR\", Pipeline([(\"scaler\", StandardScaler()), (\"m\", LinearRegression())])),\n",
                "    (\"LASSO\", Pipeline([(\"scaler\", StandardScaler()), (\"m\", Lasso(alpha=0.01, max_iter=5000))])),\n",
                "    (\"EN\", Pipeline([(\"scaler\", StandardScaler()), (\"m\", ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=5000))])),\n",
                "    (\"KNN\", Pipeline([(\"scaler\", StandardScaler()), (\"m\", KNeighborsRegressor(n_neighbors=7))])),\n",
                "    (\"CART\", DecisionTreeRegressor(random_state=seed)),\n",
                "    (\"SVR\", Pipeline([(\"scaler\", StandardScaler()), (\"m\", SVR(C=2.0, epsilon=0.02, kernel=\"rbf\"))])),\n",
                "    (\"ABR\", AdaBoostRegressor(random_state=seed)),\n",
                "    (\"GBR\", GradientBoostingRegressor(random_state=seed)),\n",
                "    (\"RFR\", RandomForestRegressor(n_estimators=100, random_state=seed, n_jobs=-1)),\n",
                "    (\"ETR\", ExtraTreesRegressor(n_estimators=100, random_state=seed, n_jobs=-1)),\n",
                "]\n",
                "\n",
                "results, names = [], []\n",
                "for name, model in models:\n",
                "    cv_scores = cross_val_score(model, X_train, Y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
                "    results.append(cv_scores)\n",
                "    names.append(name)\n",
                "    print(f\"{name}: R2 mean={cv_scores.mean():.3f}  std={cv_scores.std():.3f}\")\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.boxplot([np.maximum(res, .45) for res in results], labels=names, showmeans=True)\n",
                "plt.title(\"Algorithm Comparison (10-fold CV, R²)\")\n",
                "plt.ylabel(\"R² (higher is better)\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Question:**\n",
                "- Which model ranks best by mean CV R²? Are tree ensembles clearly ahead of linear baselines?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 5 — Model Tuning & Grid Search\n",
                "\n",
                "Let's tune RandomForestRegressor. We'll start with n_estimators (number of trees) as in the original case study, then (optionally) try a tiny grid on max_depth."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import GridSearchCV\n",
                "\n",
                "rf = RandomForestRegressor(random_state=seed, n_jobs=-1)\n",
                "param_grid = {\n",
                "    \"n_estimators\": [50, 100, 150, 200],\n",
                "    # Optional small add-on:\n",
                "    \"max_depth\": [None, 6, 10],\n",
                "}\n",
                "grid = GridSearchCV(\n",
                "    estimator=rf,\n",
                "    param_grid=param_grid,\n",
                "    scoring=scoring,\n",
                "    cv=cv,\n",
                "    n_jobs=-1\n",
                ")\n",
                "grid_result = grid.fit(X_train, Y_train)\n",
                "\n",
                "print(f\"Best CV R²: {grid_result.best_score_:.6f} using {grid_result.best_params_}\")\n",
                "\n",
                "means = grid_result.cv_results_[\"mean_test_score\"]\n",
                "stds = grid_result.cv_results_[\"std_test_score\"]\n",
                "params = grid_result.cv_results_[\"params\"]\n",
                "for mean, stdev, param in zip(means, stds, params):\n",
                "    print(f\"{mean:.6f} ({stdev:.6f}) with: {param}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Question:**\n",
                "- When would you tune max_depth, max_features, or min_samples_leaf?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 6 — Finalize the Model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.1 Fit on train, evaluate on both train and validation\n",
                "\n",
                "We'll refit the best Random Forest and check performance. (Train R² is expected to be high for RF; the key is the validation R².)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
                "import numpy as np\n",
                "\n",
                "best_rf = grid_result.best_estimator_\n",
                "best_rf.fit(X_train, Y_train)\n",
                "\n",
                "# Train performance (expect high R²)\n",
                "pred_train = best_rf.predict(X_train)\n",
                "print(\"Train R²:\", r2_score(Y_train, pred_train))\n",
                "\n",
                "# Validation performance\n",
                "pred_val = best_rf.predict(X_validation)\n",
                "print(\"Validation R²:\", r2_score(Y_validation, pred_val))\n",
                "print(\"Validation RMSE:\", mean_squared_error(Y_validation, pred_val))\n",
                "print(\"Validation MAE:\", mean_absolute_error(Y_validation, pred_val))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Question:**\n",
                "- If validation R² were disappointing, what would you try next?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 Feature importance & business intuition\n",
                "\n",
                "Let's examine which variables drive the RF predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fi = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=True)\n",
                "\n",
                "plt.figure(figsize=(8, 5))\n",
                "fi.tail(10).plot(kind=\"barh\")\n",
                "plt.title(\"Top Feature Importances (Random Forest)\")\n",
                "plt.xlabel(\"Importance\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "fi.sort_values(ascending=False).head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Questions:**\n",
                "- Do importances align with the correlations you observed earlier?\n",
                "- Which of these features might need governance/ethics review before production use?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.3 Save & reload the model (for the robo-advisor)\n",
                "\n",
                "Persist the trained estimator so a dashboard can load it later."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "\n",
                "FILENAME = \"your_model.sav\"\n",
                "with open(FILENAME, \"wb\") as f:\n",
                "    pickle.dump(best_rf, f)\n",
                "\n",
                "# Load test\n",
                "with open(FILENAME, \"rb\") as f:\n",
                "    loaded_model = pickle.load(f)\n",
                "\n",
                "# Quick check on the validation set\n",
                "pred_val_loaded = loaded_model.predict(X_validation)\n",
                "print(\"Reloaded model R²:\", r2_score(Y_validation, pred_val_loaded))\n",
                "print(\"Reloaded model RMSE:\", mean_squared_error(Y_validation, pred_val_loaded))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Question:**\n",
                "- Why is it essential to save the preprocessing steps with the model when you have them?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 7 — Run the Dashboard App\n",
                "\n",
                "Run the code in terminal with your conda env:\n",
                "\n",
                "Get all packages first then run the python script app_pretty:\n",
                "\n",
                "```bash\n",
                "pip install dash\n",
                "pip install dash-core-components\n",
                "pip install dash-html-components\n",
                "pip install dash-daq\n",
                "pip install cvxopt\n",
                "pip install dash_bootstrap_components\n",
                "```\n",
                "\n",
                "Once you have everything then you can do:\n",
                "\n",
                "```bash\n",
                "python app_pretty.py\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}